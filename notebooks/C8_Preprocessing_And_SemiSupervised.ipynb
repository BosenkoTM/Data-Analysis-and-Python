{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Кластеризация. Предобработка и обучение с частичным привлечением учителя\n",
    "\n",
    "\n",
    "<hr>\n",
    "\n",
    "С.Ю. Папулин (papulin.study@yandex.ru)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Содержание\n",
    "\n",
    "- [Предобработка данных для задачи классификации](#Предобработка-данных-для-задачи-классификации)\n",
    "- [Обучение с частичным привлечением учителя](#Обучение-с-частичным-привлечением-учителя)\n",
    "- [Источники](#Источники)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Подключение библиотек:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функция загрузки исходного набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fashion MNIST Dataset: https://github.com/zalandoresearch/fashion-mnist\n",
    "\"\"\"\n",
    "\n",
    "from os import makedirs, remove\n",
    "from os.path import exists, join\n",
    "import gzip\n",
    "\n",
    "from sklearn.datasets.base import RemoteFileMetadata, _fetch_remote\n",
    "from sklearn.datasets import get_data_home\n",
    "from sklearn.utils import Bunch\n",
    "\n",
    "import numpy as np\n",
    "import logging\n",
    "\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "ARCHIVES = [\n",
    "    RemoteFileMetadata(\n",
    "        filename='train-images-idx3-ubyte.gz',\n",
    "        url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz',\n",
    "        checksum=('3aede38d61863908ad78613f6a32ed271626dd12800ba2636569512369268a84')),\n",
    "    RemoteFileMetadata(\n",
    "        filename='train-labels-idx1-ubyte.gz',\n",
    "        url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz',\n",
    "        checksum=('a04f17134ac03560a47e3764e11b92fc97de4d1bfaf8ba1a3aa29af54cc90845')),\n",
    "    RemoteFileMetadata(\n",
    "        filename='test-images-idx3-ubyte.gz',\n",
    "        url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz',\n",
    "        checksum=('346e55b948d973a97e58d2351dde16a484bd415d4595297633bb08f03db6a073')),\n",
    "    RemoteFileMetadata(\n",
    "        filename='test-labels-idx1-ubyte.gz',\n",
    "        url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz',\n",
    "        checksum=('67da17c76eaffca5446c3361aaab5c3cd6d1c2608764d35dfb1850b086bf8dd5'))\n",
    "]\n",
    "\n",
    "\n",
    "def fetch_fashion_mnist(data_home=None, download_if_missing=True, subset='all', return_X_y=False):  \n",
    "    \"\"\"\n",
    "    Load the Fashion MNIST dataset (classification).\n",
    "    \n",
    "    Note: Based on https://github.com/scikit-learn/scikit-learn\n",
    "    \"\"\"\n",
    "    data_home = get_data_home(data_home=data_home)\n",
    "    if not exists(data_home):\n",
    "        makedirs(data_home)\n",
    "\n",
    "    for archive in ARCHIVES:\n",
    "        filepath = join(data_home, archive.filename)\n",
    "        if not exists(filepath):\n",
    "            if not download_if_missing:\n",
    "                raise IOError(\"Data not found and `download_if_missing` is False\")\n",
    "            logger.info('Downloading Fashion mnist from {} to {}'.format(\n",
    "                archive.url, filepath))\n",
    "            archive_path = _fetch_remote(archive, dirname=data_home)\n",
    "        \n",
    "    if return_X_y:\n",
    "        \n",
    "        DESCR = '''\n",
    "            Fashion-MNIST is a dataset of Zalando's article images—consisting of \n",
    "            a training set of 60,000 examples and a test set of 10,000 examples. \n",
    "            Each example is a 28x28 grayscale image, associated with a label from \n",
    "            10 classes. We intend Fashion-MNIST to serve as a direct drop-in \n",
    "            replacement for the original MNIST dataset for benchmarking machine \n",
    "            learning algorithms. It shares the same image size and structure of \n",
    "            training and testing splits.\n",
    "            '''\n",
    "        \n",
    "        feature_names = [\n",
    "            'T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat', \n",
    "            'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot'\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        if subset == 'train':\n",
    "            X, y = _load_X_y(data_home, 'train')\n",
    "            return Bunch(\n",
    "                data=X,\n",
    "                target=y,\n",
    "                feature_names=feature_names,\n",
    "                DESCR=DESCR\n",
    "            )\n",
    "        elif subset == 'test':\n",
    "            X, y = _load_X_y(data_home, 'test')\n",
    "            return Bunch(\n",
    "                data=X,\n",
    "                target=y,\n",
    "                feature_names=feature_names,\n",
    "                DESCR=DESCR\n",
    "            )\n",
    "        X_train, y_train = _load_X_y(data_home, 'train')\n",
    "        X_test, y_test = _load_X_y(data_home, 'test')\n",
    "        return Bunch(\n",
    "            data={'train': X_train, 'test': X_test},\n",
    "            target={'train': y_train, 'test': y_test},\n",
    "            feature_names=feature_names,\n",
    "            DESCR=DESCR\n",
    "        )\n",
    "\n",
    "        \n",
    "def _load_X_y(path, subset='train'):\n",
    "    \"\"\"\n",
    "    Load MNIST data from `path`.\n",
    "    \n",
    "    Note: Based on \n",
    "    https://github.com/zalandoresearch/fashion-mnist/blob/master/utils/mnist_reader.py\n",
    "    \"\"\"\n",
    "    y_path = join(path, '{}-labels-idx1-ubyte.gz'.format(subset))\n",
    "    X_path = join(path, '{}-images-idx3-ubyte.gz'.format(subset))\n",
    "\n",
    "    with gzip.open(y_path, 'rb') as y_file:\n",
    "        y = np.frombuffer(y_file.read(), dtype=np.uint8,\n",
    "                               offset=8)\n",
    "    with gzip.open(X_path, 'rb') as X_file:\n",
    "        X = np.frombuffer(X_file.read(), dtype=np.uint8,\n",
    "                               offset=16).reshape(len(y), 784)\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Предобработка данных для задачи классификации"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset = fetch_fashion_mnist(return_X_y=True)\n",
    "\n",
    "print('Overview\\n', fashion_dataset.DESCR)\n",
    "print('Feature names\\n', fashion_dataset.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_INDX = 20\n",
    "\n",
    "print('Image:')\n",
    "plt.figure(figsize=[4, 4])\n",
    "plt.imshow(fashion_dataset.data['train'][IMAGE_INDX].reshape(-1, 28))\n",
    "plt.show()\n",
    "\n",
    "print('Target:', fashion_dataset.target['train'][IMAGE_INDX])\n",
    "print('Name:', fashion_dataset.feature_names[fashion_dataset.target['train'][IMAGE_INDX]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Размерность данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset.data['train'].shape, fashion_dataset.data['test'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_dataset.target['train'].shape, fashion_dataset.target['test'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучающее и тестовое подмножества"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_, X_test, y_train_, y_test = *fashion_dataset.data.values(), *fashion_dataset.target.values()\n",
    "\n",
    "# Уменьшение количества элементов обучающего множества\n",
    "X_train = X_train_[:10000]\n",
    "y_train = y_train_[:10000]\n",
    "\n",
    "# Уменьшение размера изображений\n",
    "# X_train = X_train.reshape(-1, 28, 28)[:, ::2, ::2].reshape(-1, 14*14)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Обучения классификатора"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = time.time()\n",
    "model = SVC(**{'C': 10, 'kernel': 'poly', 'gamma': 'scale'})\n",
    "model.fit(X_train, y_train)\n",
    "print(\"Time =\", time.time() - tick)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Формирование признаков посредством кластеризации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = time.time()\n",
    "cluster_model = KMeans(n_clusters=50, random_state=12345)\n",
    "cluster_model.fit(X_train)\n",
    "print(\"Time =\", time.time() - tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Расстояние до центров кластеров\n",
    "cluster_model.transform(X_train)[:1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_model.transform(X_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание кластеров\n",
    "print(np.argmin(cluster_model.transform(X_train), axis=1)[:5])\n",
    "print(cluster_model.predict(X_train[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Классификатор с новым наборам признаков"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = time.time()\n",
    "model = SVC(**{'C':10, 'kernel':'poly', 'gamma': 'scale'})\n",
    "model.fit(cluster_model.transform(X_train), y_train)\n",
    "print('Time =', time.time() - tick)\n",
    "model.score(cluster_model.transform(X_test), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Реализация посредством `Pipeline`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tick = time.time()\n",
    "pipeline = Pipeline([\n",
    "    ('cluster_model', KMeans(n_clusters=50, random_state=12345)),\n",
    "    ('classifier', SVC(**{'C':10, 'kernel': 'poly', 'gamma': 'scale'})),\n",
    "])\n",
    "pipeline.fit(X_train, y_train)\n",
    "print('Time =', time.time() - tick)\n",
    "pipeline.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Обучение с частичным привлечением учителя"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Везьмем первые n наблюдений\n",
    "N = 500\n",
    "X_train_n = X_train[:N]\n",
    "y_train_n = y_train[:N]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модель классификации\n",
    "tick = time.time()\n",
    "model = SVC(**{'C': 10, 'kernel': 'poly', 'gamma': 'scale'})\n",
    "# model = LogisticRegression()\n",
    "model.fit(X_train_n, y_train_n)\n",
    "print('Accuracy =', model.score(X_test, y_test))\n",
    "print('Time =', time.time() - tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучаем модедь кластеризации\n",
    "tick = time.time()\n",
    "cluster_model = KMeans(n_clusters=N, random_state=12345)\n",
    "cluster_model.fit(X_train)\n",
    "print('Time =', time.time() - tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметка данных ближайших к кластерам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Индексы наблюдений с минимальным расстоянием до ближайщего кластера\n",
    "indices = np.argmin(cluster_model.transform(X_train), axis=0)\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cols = 10\n",
    "row_num = -(-len(indices) // cols)\n",
    "\n",
    "fig, axs = plt.subplots(row_num, cols, figsize=(14, 2*row_num), squeeze=False)\n",
    "for i in range(row_num):\n",
    "    for j in range(cols):\n",
    "        indx = i * cols + j\n",
    "        if indx >= len(indices):\n",
    "            fig.delaxes(axs[i, j])\n",
    "        else:\n",
    "            image = X_train[indices[indx]].reshape(-1, 28)\n",
    "            axs[i, j].imshow(image)\n",
    "            axs[i, j].set_title(\n",
    "                \"cluster={}\".format(indx))\n",
    "            axs[i, j].axis(\"off\")\n",
    "# plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Массив наблюдений, соответствующих ранее полученным индексам\n",
    "X_train_n_labeled = X_train[indices]\n",
    "\n",
    "# Замечения: Эти значения должны быть внесены вручную на основе\n",
    "# изображений выше. Однако здесь мы используем уже размеченный \n",
    "# набор с целевыми значениями\n",
    "y_train_n_labeled = y_train[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение на новом наборе из N размеченных данных\n",
    "model = SVC(**{'C': 10, 'kernel': 'poly', 'gamma': 'scale'})\n",
    "model.fit(X_train_n_labeled, y_train_n_labeled)\n",
    "print('Accuracy =', model.score(X_test, y_test))\n",
    "print('Time =', time.time() - tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Разметка всего набора данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Предсказание кластеров\n",
    "с__pred = cluster_model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Сопоставим индексы кластеров и индексы классов (для размеченных вручную изображений)\n",
    "y_train_labeled = y_train[indices[с__pred]]\n",
    "y_train_labeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучение на новых размеченных данных\n",
    "tick = time.time()\n",
    "model = SVC(**{'C': 10, 'kernel': 'poly', 'gamma': 'scale'})\n",
    "model.fit(X_train, y_train_labeled)\n",
    "print('Accuracy =', model.score(X_test, y_test))\n",
    "print('Time =', time.time() - tick)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Приведенные выше способы подходят для повышения качества предсказания моделей при небольшом количестве размеченных данных. Если у нас достаточно большой набор размеченных данных, то не стоит ожидать значительного увеличения качества предсказания"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Источники"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hands-on Machine Learning with Scikit-Learn, Keras, and TensorFlow by Aurélien Géron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
